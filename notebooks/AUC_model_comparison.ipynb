{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_rf</th>\n",
       "      <th>pred_gb</th>\n",
       "      <th>pred_lr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.157483</td>\n",
       "      <td>0.050373</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007528</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>0.021629</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010931</td>\n",
       "      <td>0.019073</td>\n",
       "      <td>0.013231</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014331</td>\n",
       "      <td>0.026936</td>\n",
       "      <td>0.109715</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.005357</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred_rf   pred_gb   pred_lr  label\n",
       "0  0.157483  0.050373  0.012200  False\n",
       "1  0.007528  0.017296  0.021629  False\n",
       "2  0.010931  0.019073  0.013231  False\n",
       "3  0.014331  0.026936  0.109715  False\n",
       "4  0.002645  0.017125  0.005357  False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the prediction scores on the test set accross all three models\n",
    "test_multiple_models = pd.read_csv('../data/test_score_multiple_models.csv')\n",
    "test_multiple_models.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test\n",
    "- randomly select with replacement from the test set\n",
    "- Compute the auc for each model\n",
    "- Compute the proportion of times model A > model B: $P(AUC_A > AUC_B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "N = test_multiple_models.shape[0]\n",
    "bootstraps = 10000\n",
    "models = ['rf', 'gb', 'lr']\n",
    "res_list = []\n",
    "for b in range(bootstraps):\n",
    "    idx = np.random.choice(range(N), N, replace=True)\n",
    "    res = {}\n",
    "    for model in models:\n",
    "        tmp = test_multiple_models.iloc[idx]\n",
    "        probas = tmp['pred_'+model]\n",
    "        y = tmp['label']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y, probas)\n",
    "        try:\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "        except:\n",
    "            auc = np.nan\n",
    "        res.update({model: auc})\n",
    "    res_list.append(res)\n",
    "auc_bootstrapped = pd.DataFrame(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf 0.8753544399213361 [0.85685892 0.89239396]\n",
      "gb 0.8674011848429615 [0.84809157 0.88577818]\n",
      "lr 0.8560106915352218 [0.83510814 0.87601817]\n"
     ]
    }
   ],
   "source": [
    "# Mean AUCs and CIs\n",
    "for model in models:\n",
    "    print(model, auc_bootstrapped[model].mean(), np.percentile(auc_bootstrapped[model], [2.5, 97.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(AUC_{linear regression} > AUC_{random forest})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(auc_bootstrapped['lr'] > auc_bootstrapped['rf']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(AUC_{linear regression} > AUC_{gradient boosting})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1213"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(auc_bootstrapped['lr'] > auc_bootstrapped['gb']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(AUC_{gradient boosting} > AUC_{random forest})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(auc_bootstrapped['gb'] > auc_bootstrapped['rf']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bonnferoni correction, \n",
    "$$\\alpha_{\\{per comparison\\}} = \\alpha/m \\\\\n",
    "= 0.05/3 \\\\\n",
    "= 0.017$$\n",
    "\n",
    "Conclusion that taking into account the multiple comparisons, the difference between the best and worst performing models is $p < \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
